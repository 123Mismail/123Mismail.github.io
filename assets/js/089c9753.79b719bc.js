"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[951],{8453(e,n,t){t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},8703(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>_});const i=JSON.parse('{"id":"c11-sensor-fusion","title":"Chapter 11: Sensor Fusion for Humanoid Robotics","description":"Learning Outcomes","source":"@site/chapters/c11-sensor-fusion.md","sourceDirName":".","slug":"/c11-sensor-fusion","permalink":"/chapters/c11-sensor-fusion","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"id":"c11-sensor-fusion","title":"Chapter 11: Sensor Fusion for Humanoid Robotics","sidebar_label":"C11: Sensor Fusion","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"C10: Real-Time Algorithms","permalink":"/chapters/c10-real-time-algorithms"},"next":{"title":"Chapter 12: Whole-Body Control for Humanoid Robotics","permalink":"/chapters/c12-whole-body-control"}}');var a=t(4848),o=t(8453);const r={id:"c11-sensor-fusion",title:"Chapter 11: Sensor Fusion for Humanoid Robotics",sidebar_label:"C11: Sensor Fusion",sidebar_position:11},s="Chapter 11: Sensor Fusion for Humanoid Robotics",c={},_=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Kalman Filtering for Humanoid State Estimation",id:"kalman-filtering-for-humanoid-state-estimation",level:2},{value:"Extended Kalman Filter Implementation",id:"extended-kalman-filter-implementation",level:3},{value:"Unscented Kalman Filter for Nonlinear State Estimation",id:"unscented-kalman-filter-for-nonlinear-state-estimation",level:3},{value:"Visual-Inertial Odometry (VIO) for Humanoid Localization",id:"visual-inertial-odometry-vio-for-humanoid-localization",level:2},{value:"VIO Feature Tracking and State Estimation",id:"vio-feature-tracking-and-state-estimation",level:3},{value:"Multi-Sensor Integration Architecture",id:"multi-sensor-integration-architecture",level:2},{value:"Sensor Fusion Framework for Humanoid Robots",id:"sensor-fusion-framework-for-humanoid-robots",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-11-sensor-fusion-for-humanoid-robotics",children:"Chapter 11: Sensor Fusion for Humanoid Robotics"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement"})," Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) for humanoid state estimation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Design"})," multi-sensor fusion architectures that integrate IMU, vision, and proprioceptive sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deploy"})," visual-inertial odometry (VIO) systems for humanoid localization and mapping"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Evaluate"})," sensor fusion performance in terms of accuracy, latency, and robustness for real-time applications"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.p,{children:["Chapters 9 and 10 established real-time control systems and algorithms for humanoid robotics. However, effective control requires accurate state estimation from multiple sensors. ",(0,a.jsx)(n.strong,{children:"Sensor fusion"})," combines data from various sensors (IMUs, cameras, encoders, force/torque sensors) to provide robust, accurate state estimates for humanoid control systems."]}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots operate in complex, dynamic environments where individual sensors have limitations: IMUs drift over time, cameras fail in poor lighting, and encoders don't provide absolute position. Sensor fusion algorithms address these limitations by combining complementary sensor information, providing robust state estimates for balance control, navigation, and manipulation tasks."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Visual-Inertial Odometry (VIO)"})," represents a key fusion approach, combining camera visual features with IMU inertial measurements for accurate pose estimation. ",(0,a.jsx)(n.strong,{children:"Kalman filtering"})," provides the mathematical foundation for optimal state estimation under uncertainty. ",(0,a.jsx)(n.strong,{children:"Multi-sensor architectures"})," integrate diverse sensor types into unified estimation frameworks."]}),"\n",(0,a.jsx)(n.p,{children:"This chapter explores sensor fusion techniques for humanoid robotics, focusing on real-time implementation, computational efficiency, and robust performance. You will learn to implement EKF/UKF estimators, design VIO systems, and optimize fusion algorithms for embedded deployment."}),"\n",(0,a.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Fusion"}),": Process of combining data from multiple sensors to achieve more accurate and reliable state estimates than individual sensors can provide"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Extended Kalman Filter (EKF)"}),": Nonlinear Kalman filter variant that linearizes system models around current state estimates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unscented Kalman Filter (UKF)"}),": Nonlinear filter that uses deterministic sampling to capture state distribution more accurately than EKF"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual-Inertial Odometry (VIO)"}),": Estimation technique combining visual features and inertial measurements for pose tracking"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Estimation"}),": Process of determining robot state (position, orientation, velocity, etc.) from sensor measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-rate Fusion"}),": Technique for fusing sensors with different update rates (e.g., IMU at 1000Hz, cameras at 30Hz)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Observability"}),": Property of sensor systems indicating whether state variables can be uniquely determined from sensor measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Covariance"}),": Matrix representing uncertainty in state estimates, essential for optimal fusion"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"kalman-filtering-for-humanoid-state-estimation",children:"Kalman Filtering for Humanoid State Estimation"}),"\n",(0,a.jsx)(n.p,{children:"Kalman filters provide optimal state estimation for linear systems with Gaussian noise, while EKF and UKF extend this to nonlinear systems:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Sensor Data    \u2502\u2500\u2500\u2500\u25ba\u2502  Kalman Filter   \u2502\u2500\u2500\u2500\u25ba\u2502  Estimated State \u2502\n\u2502  (IMU, Vision,  \u2502    \u2502  (Prediction &   \u2502    \u2502  (Position,      \u2502\n\u2502   Encoders)     \u2502    \u2502   Update)        \u2502    \u2502   Orientation,   \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502   Velocity)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                        \u2502\n         \u25bc                       \u25bc                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Multi-rate     \u2502    \u2502  Covariance      \u2502    \u2502  < 10ms          \u2502\n\u2502  Data Input     \u2502    \u2502  Management      \u2502    \u2502  Update Time     \u2502\n\u2502  (1000Hz IMU,   \u2502    \u2502  (Uncertainty    \u2502    \u2502                  \u2502\n\u2502   100Hz Vision) \u2502    \u2502   Propagation)   \u2502    \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.p,{children:"Kalman filters optimally combine sensor measurements with motion models to minimize estimation uncertainty, making them ideal for humanoid state estimation where multiple sensors provide complementary information."}),"\n",(0,a.jsx)(n.h3,{id:"extended-kalman-filter-implementation",children:"Extended Kalman Filter Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:"// ekf_state_estimator.cpp - Extended Kalman Filter for humanoid state estimation\n#include <Eigen/Dense>\n#include <vector>\n#include <chrono>\n\nnamespace humanoid_sensors {\n\nclass EKFStateEstimator {\nprivate:\n    // State vector: [px, py, pz, qx, qy, qz, qw, vx, vy, vz, wx, wy, wz]\n    // Position (3), Orientation quaternion (4), Linear velocity (3), Angular velocity (3)\n    static constexpr int STATE_DIM = 13;\n    static constexpr int IMU_MEAS_DIM = 6;  // Accelerometer (3) + Gyroscope (3)\n    static constexpr int POS_MEAS_DIM = 3;  // Position measurement (3)\n\n    // State vector\n    Eigen::VectorXd state_;\n    Eigen::MatrixXd covariance_;\n\n    // Process noise and measurement noise\n    Eigen::MatrixXd process_noise_;\n    Eigen::MatrixXd imu_noise_;\n    Eigen::MatrixXd pos_noise_;\n\n    // Time tracking\n    double last_update_time_;\n    bool initialized_;\n\n    // Gravity vector in world frame\n    Eigen::Vector3d gravity_world_;\n\npublic:\n    EKFStateEstimator() : initialized_(false) {\n        state_ = Eigen::VectorXd::Zero(STATE_DIM);\n        covariance_ = Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM) * 0.1;\n\n        // Initialize with identity quaternion for orientation\n        state_(6) = 1.0;  // qw component\n\n        // Process noise matrix (tuned for humanoid dynamics)\n        process_noise_ = Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM);\n        process_noise_.block<3,3>(0,0) *= 0.1;    // Position process noise\n        process_noise_.block<4,4>(3,3) *= 0.01;   // Orientation process noise\n        process_noise_.block<3,3>(7,7) *= 0.5;    // Velocity process noise\n        process_noise_.block<3,3>(10,10) *= 0.2;  // Angular velocity process noise\n\n        // Measurement noise matrices\n        imu_noise_ = Eigen::MatrixXd::Identity(IMU_MEAS_DIM, IMU_MEAS_DIM) * 0.01;\n        pos_noise_ = Eigen::MatrixXd::Identity(POS_MEAS_DIM, POS_MEAS_DIM) * 0.05;\n\n        gravity_world_ << 0.0, 0.0, -9.81;\n        last_update_time_ = 0.0;\n    }\n\n    struct IMUData {\n        Eigen::Vector3d accelerometer;\n        Eigen::Vector3d gyroscope;\n        double timestamp;\n    };\n\n    struct PositionData {\n        Eigen::Vector3d position;\n        double timestamp;\n    };\n\n    void initialize_state(const Eigen::Vector3d& initial_pos,\n                         const Eigen::Quaterniond& initial_orientation,\n                         double timestamp) {\n        state_.segment<3>(0) = initial_pos;  // Position\n        state_.segment<4>(3) << initial_orientation.x(),\n                                initial_orientation.y(),\n                                initial_orientation.z(),\n                                initial_orientation.w();  // Orientation quaternion\n        state_.segment<3>(7) = Eigen::Vector3d::Zero();  // Velocity\n        state_.segment<3>(10) = Eigen::Vector3d::Zero(); // Angular velocity\n\n        last_update_time_ = timestamp;\n        initialized_ = true;\n    }\n\n    void predict(const IMUData& imu_data) {\n        if (!initialized_) return;\n\n        double dt = imu_data.timestamp - last_update_time_;\n        if (dt <= 0) return;  // Invalid time\n\n        // Extract state components\n        Eigen::Vector3d pos = state_.segment<3>(0);\n        Eigen::Quaterniond orientation(state_(6), state_(3), state_(4), state_(5)); // w, x, y, z\n        Eigen::Vector3d vel = state_.segment<3>(7);\n        Eigen::Vector3d ang_vel = state_.segment<3>(10);\n\n        // Normalize quaternion to prevent drift\n        orientation.normalize();\n\n        // Prediction step: integrate motion model\n        // 1. Update orientation based on angular velocity\n        Eigen::Quaterniond orientation_rate =\n            compute_quaternion_derivative(orientation, imu_data.gyroscope);\n        Eigen::Vector4d orientation_dot = Eigen::Vector4d(\n            orientation_rate.w(),\n            orientation_rate.x(),\n            orientation_rate.y(),\n            orientation_rate.z()\n        );\n        Eigen::Vector4d new_orientation = state_.segment<4>(3) + dt * orientation_dot.segment<4>(0);\n\n        // Normalize quaternion\n        Eigen::Quaterniond new_quat(new_orientation(0), new_orientation(1),\n                                   new_orientation(2), new_orientation(3));\n        new_quat.normalize();\n        state_.segment<4>(3) << new_quat.x(), new_quat.y(), new_quat.z(), new_quat.w();\n\n        // 2. Compute acceleration in world frame\n        Eigen::Matrix3d rot_matrix = new_quat.toRotationMatrix();\n        Eigen::Vector3d acceleration_world = rot_matrix * imu_data.accelerometer + gravity_world_;\n\n        // 3. Update velocity and position\n        Eigen::Vector3d new_vel = vel + dt * acceleration_world;\n        Eigen::Vector3d new_pos = pos + dt * new_vel;\n\n        // Update state vector\n        state_.segment<3>(0) = new_pos;    // Position\n        state_.segment<3>(7) = new_vel;    // Velocity\n        state_.segment<3>(10) = imu_data.gyroscope;  // Angular velocity (measured)\n\n        // Update covariance matrix (simplified - in practice, use Jacobians)\n        Eigen::MatrixXd F = compute_state_transition_jacobian(imu_data, dt);\n        covariance_ = F * covariance_ * F.transpose() + process_noise_ * dt;\n\n        last_update_time_ = imu_data.timestamp;\n    }\n\n    void update_position(const PositionData& pos_data) {\n        if (!initialized_) return;\n\n        // Measurement model: extract position from state\n        Eigen::VectorXd h = Eigen::VectorXd::Zero(POS_MEAS_DIM);\n        h = state_.segment<3>(0);  // Position is first 3 elements\n\n        // Measurement Jacobian (identity for position measurement)\n        Eigen::MatrixXd H = Eigen::MatrixXd::Zero(POS_MEAS_DIM, STATE_DIM);\n        H.block<3,3>(0, 0) = Eigen::Matrix3d::Identity();\n\n        // Innovation and Kalman gain calculation\n        Eigen::VectorXd innovation = pos_data.position - h;\n        Eigen::MatrixXd S = H * covariance_ * H.transpose() + pos_noise_;\n        Eigen::MatrixXd K = covariance_ * H.transpose() * S.inverse();\n\n        // Update state and covariance\n        state_ = state_ + K * innovation;\n        covariance_ = (Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM) - K * H) * covariance_;\n\n        // Re-normalize quaternion after update\n        Eigen::Quaterniond quat(state_(6), state_(3), state_(4), state_(5));\n        quat.normalize();\n        state_.segment<4>(3) << quat.x(), quat.y(), quat.z(), quat.w();\n    }\n\n    void update_imu(const IMUData& imu_data) {\n        // For IMU data, we typically use it in the prediction step\n        // This method could handle outlier detection or additional processing\n        predict(imu_data);\n    }\n\n    // Real-time safe accessors\n    Eigen::Vector3d get_position() const {\n        return state_.segment<3>(0);\n    }\n\n    Eigen::Quaterniond get_orientation() const {\n        Eigen::Quaterniond quat(state_(6), state_(3), state_(4), state_(5));\n        return quat;\n    }\n\n    Eigen::Vector3d get_velocity() const {\n        return state_.segment<3>(7);\n    }\n\n    Eigen::Vector3d get_angular_velocity() const {\n        return state_.segment<3>(10);\n    }\n\n    Eigen::MatrixXd get_covariance() const {\n        return covariance_;\n    }\n\nprivate:\n    Eigen::Quaterniond compute_quaternion_derivative(\n        const Eigen::Quaterniond& q,\n        const Eigen::Vector3d& omega) const {\n\n        // Quaternion derivative: q_dot = 0.5 * Omega * q\n        // where Omega is the skew-symmetric matrix of angular velocity\n        Eigen::Quaterniond omega_quat(0.0, omega.x(), omega.y(), omega.z());\n        return 0.5 * (q * omega_quat);\n    }\n\n    Eigen::MatrixXd compute_state_transition_jacobian(const IMUData& imu_data, double dt) const {\n        Eigen::MatrixXd F = Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM);\n\n        // Linearize the motion model around current state\n        // This is a simplified version - full implementation would include\n        // proper Jacobian computation for the nonlinear motion model\n\n        // Position-velocity relationship\n        F.block<3,3>(0, 7) = dt * Eigen::Matrix3d::Identity();\n\n        // Velocity-acceleration relationship (approximate)\n        Eigen::Quaterniond current_quat(state_(6), state_(3), state_(4), state_(5));\n        Eigen::Matrix3d rot_matrix = current_quat.toRotationMatrix();\n\n        // The Jacobian of rotation matrix with respect to orientation is complex\n        // This is a simplified approximation\n        F.block<3,3>(7, 3) = dt * compute_rotation_jacobian(imu_data.accelerometer);\n\n        return F;\n    }\n\n    Eigen::Matrix3d compute_rotation_jacobian(const Eigen::Vector3d& acc) const {\n        // Simplified Jacobian computation\n        // In practice, this would involve the full derivative of the rotation matrix\n        return Eigen::Matrix3d::Zero();\n    }\n};\n\n} // namespace humanoid_sensors\n"})}),"\n",(0,a.jsx)(n.h3,{id:"unscented-kalman-filter-for-nonlinear-state-estimation",children:"Unscented Kalman Filter for Nonlinear State Estimation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:"// ukf_state_estimator.cpp - Unscented Kalman Filter for humanoid state estimation\n#include <Eigen/Dense>\n#include <vector>\n\nnamespace humanoid_sensors {\n\nclass UKFStateEstimator {\nprivate:\n    static constexpr int STATE_DIM = 13;\n    static constexpr int MEAS_DIM = 6;  // IMU measurement dimension\n\n    // State vector and covariance\n    Eigen::VectorXd state_;\n    Eigen::MatrixXd covariance_;\n\n    // Sigma points\n    std::vector<Eigen::VectorXd> sigma_points_;\n    std::vector<double> weights_mean_;\n    std::vector<double> weights_covariance_;\n\n    // UKF parameters\n    double alpha_;\n    double beta_;\n    double kappa_;\n    int lambda_;\n    int num_sigma_points_;\n\n    // Process and measurement noise\n    Eigen::MatrixXd process_noise_;\n    Eigen::MatrixXd measurement_noise_;\n\n    // Time tracking\n    double last_update_time_;\n    bool initialized_;\n\n    // Gravity vector\n    Eigen::Vector3d gravity_world_;\n\npublic:\n    UKFStateEstimator() : alpha_(0.001), beta_(2.0), kappa_(0.0), initialized_(false) {\n        // Calculate UKF parameters\n        lambda_ = alpha_ * alpha_ * (STATE_DIM + kappa_) - STATE_DIM;\n        num_sigma_points_ = 2 * STATE_DIM + 1;\n\n        // Initialize state and covariance\n        state_ = Eigen::VectorXd::Zero(STATE_DIM);\n        state_(6) = 1.0;  // Initialize quaternion w component\n        covariance_ = Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM) * 0.1;\n\n        // Initialize sigma points\n        sigma_points_.resize(num_sigma_points_);\n        for (auto& point : sigma_points_) {\n            point = Eigen::VectorXd::Zero(STATE_DIM);\n        }\n\n        // Initialize weights\n        weights_mean_.resize(num_sigma_points_);\n        weights_covariance_.resize(num_sigma_points_);\n\n        double weight_0 = lambda_ / (STATE_DIM + lambda_);\n        weights_mean_[0] = weight_0;\n        weights_covariance_[0] = weight_0 + (1 - alpha_ * alpha_ + beta_);\n\n        for (int i = 1; i < num_sigma_points_; ++i) {\n            weights_mean_[i] = 1.0 / (2 * (STATE_DIM + lambda_));\n            weights_covariance_[i] = weights_mean_[i];\n        }\n\n        // Initialize noise matrices\n        process_noise_ = Eigen::MatrixXd::Identity(STATE_DIM, STATE_DIM) * 0.01;\n        measurement_noise_ = Eigen::MatrixXd::Identity(MEAS_DIM, MEAS_DIM) * 0.01;\n\n        gravity_world_ << 0.0, 0.0, -9.81;\n        last_update_time_ = 0.0;\n    }\n\n    void initialize_state(const Eigen::Vector3d& initial_pos,\n                         const Eigen::Quaterniond& initial_orientation,\n                         double timestamp) {\n        state_.segment<3>(0) = initial_pos;  // Position\n        state_.segment<4>(3) << initial_orientation.x(),\n                                initial_orientation.y(),\n                                initial_orientation.z(),\n                                initial_orientation.w();  // Orientation quaternion\n        state_.segment<3>(7) = Eigen::Vector3d::Zero();  // Velocity\n        state_.segment<3>(10) = Eigen::Vector3d::Zero(); // Angular velocity\n\n        last_update_time_ = timestamp;\n        initialized_ = true;\n\n        // Generate initial sigma points\n        generate_sigma_points();\n    }\n\n    void predict(double dt) {\n        if (!initialized_ || dt <= 0) return;\n\n        // Generate sigma points based on current state and covariance\n        generate_sigma_points();\n\n        // Propagate each sigma point through the motion model\n        std::vector<Eigen::VectorXd> propagated_points(num_sigma_points_);\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            propagated_points[i] = propagate_state(sigma_points_[i], dt);\n        }\n\n        // Calculate predicted state and covariance\n        state_ = Eigen::VectorXd::Zero(STATE_DIM);\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            state_ += weights_mean_[i] * propagated_points[i];\n        }\n\n        // Re-normalize quaternion\n        Eigen::Quaterniond quat(state_(6), state_(3), state_(4), state_(5));\n        quat.normalize();\n        state_.segment<4>(3) << quat.x(), quat.y(), quat.z(), quat.w();\n\n        // Calculate predicted covariance\n        covariance_ = Eigen::MatrixXd::Zero(STATE_DIM, STATE_DIM);\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            Eigen::VectorXd diff = propagated_points[i] - state_;\n            covariance_ += weights_covariance_[i] * diff * diff.transpose();\n        }\n        covariance_ += process_noise_ * dt;\n    }\n\n    void update(const Eigen::VectorXd& measurement) {\n        if (!initialized_) return;\n\n        // Generate sigma points based on predicted state\n        generate_sigma_points();\n\n        // Propagate sigma points through measurement model\n        std::vector<Eigen::VectorXd> measurement_points(num_sigma_points_);\n        Eigen::VectorXd predicted_measurement = Eigen::VectorXd::Zero(MEAS_DIM);\n\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            measurement_points[i] = measurement_model(sigma_points_[i]);\n            predicted_measurement += weights_mean_[i] * measurement_points[i];\n        }\n\n        // Calculate innovation covariance\n        Eigen::MatrixXd innovation_cov = Eigen::MatrixXd::Zero(MEAS_DIM, MEAS_DIM);\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            Eigen::VectorXd diff = measurement_points[i] - predicted_measurement;\n            innovation_cov += weights_covariance_[i] * diff * diff.transpose();\n        }\n        innovation_cov += measurement_noise_;\n\n        // Calculate cross-covariance\n        Eigen::MatrixXd cross_cov = Eigen::MatrixXd::Zero(STATE_DIM, MEAS_DIM);\n        for (int i = 0; i < num_sigma_points_; ++i) {\n            Eigen::VectorXd state_diff = sigma_points_[i] - state_;\n            Eigen::VectorXd meas_diff = measurement_points[i] - predicted_measurement;\n            cross_cov += weights_covariance_[i] * state_diff * meas_diff.transpose();\n        }\n\n        // Calculate Kalman gain\n        Eigen::MatrixXd kalman_gain = cross_cov * innovation_cov.inverse();\n\n        // Update state and covariance\n        Eigen::VectorXd innovation = measurement - predicted_measurement;\n        state_ = state_ + kalman_gain * innovation;\n\n        // Re-normalize quaternion after update\n        Eigen::Quaterniond quat(state_(6), state_(3), state_(4), state_(5));\n        quat.normalize();\n        state_.segment<4>(3) << quat.x(), quat.y(), quat.z(), quat.w();\n\n        covariance_ = covariance_ - kalman_gain * innovation_cov * kalman_gain.transpose();\n    }\n\n    // Real-time safe accessors\n    Eigen::Vector3d get_position() const {\n        return state_.segment<3>(0);\n    }\n\n    Eigen::Quaterniond get_orientation() const {\n        Eigen::Quaterniond quat(state_(6), state_(3), state_(4), state_(5));\n        return quat;\n    }\n\n    Eigen::Vector3d get_velocity() const {\n        return state_.segment<3>(7);\n    }\n\n    Eigen::Vector3d get_angular_velocity() const {\n        return state_.segment<3>(10);\n    }\n\n    Eigen::MatrixXd get_covariance() const {\n        return covariance_;\n    }\n\nprivate:\n    void generate_sigma_points() {\n        // Calculate square root of covariance matrix\n        Eigen::MatrixXd sqrt_cov = covariance_.llt().matrixL();\n\n        // First sigma point is the mean\n        sigma_points_[0] = state_;\n\n        // Generate remaining sigma points\n        for (int i = 0; i < STATE_DIM; ++i) {\n            Eigen::VectorXd offset = std::sqrt(STATE_DIM + lambda_) * sqrt_cov.col(i);\n\n            sigma_points_[i + 1] = state_ + offset;\n            sigma_points_[i + 1 + STATE_DIM] = state_ - offset;\n\n            // Normalize quaternion for each sigma point\n            Eigen::Quaterniond quat(sigma_points_[i + 1](6),\n                                   sigma_points_[i + 1](3),\n                                   sigma_points_[i + 1](4),\n                                   sigma_points_[i + 1](5));\n            quat.normalize();\n            sigma_points_[i + 1].segment<4>(3) << quat.x(), quat.y(), quat.z(), quat.w();\n\n            quat = Eigen::Quaterniond(sigma_points_[i + 1 + STATE_DIM](6),\n                                     sigma_points_[i + 1 + STATE_DIM](3),\n                                     sigma_points_[i + 1 + STATE_DIM](4),\n                                     sigma_points_[i + 1 + STATE_DIM](5));\n            quat.normalize();\n            sigma_points_[i + 1 + STATE_DIM].segment<4>(3) << quat.x(), quat.y(), quat.z(), quat.w();\n        }\n    }\n\n    Eigen::VectorXd propagate_state(const Eigen::VectorXd& state_in, double dt) const {\n        Eigen::VectorXd state_out = state_in;\n\n        // Extract components\n        Eigen::Vector3d pos = state_in.segment<3>(0);\n        Eigen::Quaterniond orientation(state_in(6), state_in(3), state_in(4), state_in(5));\n        Eigen::Vector3d vel = state_in.segment<3>(7);\n        Eigen::Vector3d ang_vel = state_in.segment<3>(10);\n\n        // Normalize quaternion\n        orientation.normalize();\n\n        // Compute rotation matrix\n        Eigen::Matrix3d rot_matrix = orientation.toRotationMatrix();\n\n        // Compute acceleration in world frame (simplified - assumes IMU measurement is available)\n        Eigen::Vector3d acceleration_world = rot_matrix * Eigen::Vector3d(0, 0, 0) + gravity_world_;\n\n        // Integrate: position, velocity, orientation\n        Eigen::Vector3d new_pos = pos + dt * vel;\n        Eigen::Vector3d new_vel = vel + dt * acceleration_world;\n\n        // Update orientation using angular velocity\n        Eigen::Quaterniond orientation_rate = compute_quaternion_derivative(orientation, ang_vel);\n        Eigen::Quaterniond new_orientation = orientation + dt * orientation_rate;\n        new_orientation.normalize();\n\n        // Update state\n        state_out.segment<3>(0) = new_pos;\n        state_out.segment<3>(7) = new_vel;\n        state_out.segment<4>(3) << new_orientation.x(), new_orientation.y(),\n                                  new_orientation.z(), new_orientation.w();\n\n        return state_out;\n    }\n\n    Eigen::VectorXd measurement_model(const Eigen::VectorXd& state_in) const {\n        // Simplified measurement model - returns IMU-like measurements\n        // In practice, this would depend on the specific sensor configuration\n        Eigen::VectorXd measurement = Eigen::VectorXd::Zero(MEAS_DIM);\n\n        // Return accelerometer and gyroscope measurements\n        // (In real implementation, this would use the actual sensor model)\n        measurement.segment<3>(0) = Eigen::Vector3d(0, 0, 0);  // Accelerometer\n        measurement.segment<3>(3) = state_in.segment<3>(10);   // Gyroscope (angular velocity)\n\n        return measurement;\n    }\n\n    Eigen::Quaterniond compute_quaternion_derivative(\n        const Eigen::Quaterniond& q,\n        const Eigen::Vector3d& omega) const {\n\n        Eigen::Quaterniond omega_quat(0.0, omega.x(), omega.y(), omega.z());\n        return 0.5 * (q * omega_quat);\n    }\n};\n\n} // namespace humanoid_sensors\n"})}),"\n",(0,a.jsx)(n.h2,{id:"visual-inertial-odometry-vio-for-humanoid-localization",children:"Visual-Inertial Odometry (VIO) for Humanoid Localization"}),"\n",(0,a.jsx)(n.p,{children:"Visual-Inertial Odometry combines camera visual features with IMU measurements for robust pose estimation:"}),"\n",(0,a.jsx)(n.h3,{id:"vio-feature-tracking-and-state-estimation",children:"VIO Feature Tracking and State Estimation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:"// vio_estimator.cpp - Visual-Inertial Odometry for humanoid localization\n#include <Eigen/Dense>\n#include <opencv2/opencv.hpp>\n#include <vector>\n#include <map>\n#include <deque>\n\nnamespace humanoid_sensors {\n\nclass VIOEstimator {\nprivate:\n    // State: [position, velocity, orientation, bias_gyro, bias_accel]\n    static constexpr int VIO_STATE_DIM = 15;\n\n    // VIO state vector: [p, v, q, bg, ba]\n    Eigen::VectorXd state_;\n    Eigen::MatrixXd covariance_;\n\n    // Feature tracking\n    std::vector<cv::Point2f> current_features_;\n    std::vector<cv::Point2f> previous_features_;\n    std::vector<bool> feature_tracked_;  // Whether feature is successfully tracked\n    std::vector<double> feature_depth_;  // Estimated depth for each feature\n    std::vector<int> feature_ids_;       // Unique IDs for tracked features\n\n    // IMU integration\n    Eigen::Vector3d integrated_velocity_;\n    Eigen::Vector3d integrated_position_;\n    Eigen::Quaterniond integrated_orientation_;\n    double integration_time_;\n\n    // VIO parameters\n    double focal_length_;\n    cv::Point2d principal_point_;\n    double reprojection_threshold_;\n    int min_feature_count_;\n    int max_feature_count_;\n\n    // Time tracking\n    double last_image_time_;\n    double last_imu_time_;\n    bool initialized_;\n\n    // Gravity vector\n    Eigen::Vector3d gravity_world_;\n\npublic:\n    VIOEstimator(double fx, double fy, double cx, double cy)\n        : focal_length_((fx + fy) / 2.0),\n          principal_point_(cx, cy),\n          reprojection_threshold_(2.0),\n          min_feature_count_(50),\n          max_feature_count_(300),\n          integration_time_(0.0),\n          initialized_(false) {\n\n        state_ = Eigen::VectorXd::Zero(VIO_STATE_DIM);\n        state_.segment<4>(6) << 0.0, 0.0, 0.0, 1.0;  // Identity quaternion\n        covariance_ = Eigen::MatrixXd::Identity(VIO_STATE_DIM, VIO_STATE_DIM) * 0.1;\n\n        gravity_world_ << 0.0, 0.0, -9.81;\n        last_image_time_ = 0.0;\n        last_imu_time_ = 0.0;\n    }\n\n    struct IMUData {\n        Eigen::Vector3d accelerometer;\n        Eigen::Vector3d gyroscope;\n        double timestamp;\n    };\n\n    struct ImageData {\n        cv::Mat image;\n        double timestamp;\n    };\n\n    void initialize_vio(const ImageData& image_data,\n                       const std::vector<IMUData>& imu_buffer) {\n        // Initialize with first image and IMU data\n        detect_features(image_data.image);\n\n        if (!imu_buffer.empty()) {\n            // Initialize orientation from accelerometer\n            Eigen::Vector3d avg_accel = Eigen::Vector3d::Zero();\n            for (const auto& imu : imu_buffer) {\n                avg_accel += imu.accelerometer;\n            }\n            avg_accel /= imu_buffer.size();\n\n            // Compute initial orientation from gravity vector\n            Eigen::Vector3d gravity_direction = avg_accel.normalized();\n            Eigen::Vector3d z_world(0, 0, 1);\n\n            // Simple initialization - in practice, use more sophisticated methods\n            Eigen::Quaterniond init_orientation = Eigen::Quaterniond::FromTwoVectors(\n                gravity_direction, -z_world);\n\n            state_.segment<4>(6) << init_orientation.x(), init_orientation.y(),\n                                  init_orientation.z(), init_orientation.w();\n        }\n\n        last_image_time_ = image_data.timestamp;\n        initialized_ = true;\n    }\n\n    void process_image(const ImageData& image_data) {\n        if (!initialized_) {\n            initialize_vio(image_data, {});\n            return;\n        }\n\n        // Track features from previous frame\n        track_features(image_data.image);\n\n        // Update state if enough features are available\n        if (current_features_.size() >= min_feature_count_) {\n            update_from_features(image_data.timestamp);\n        }\n\n        // Add new features if needed\n        if (current_features_.size() < max_feature_count_) {\n            add_new_features(image_data.image);\n        }\n\n        last_image_time_ = image_data.timestamp;\n    }\n\n    void process_imu(const IMUData& imu_data) {\n        if (!initialized_) return;\n\n        double dt = imu_data.timestamp - last_imu_time_;\n        if (dt <= 0) return;\n\n        // Remove bias from measurements\n        Eigen::Vector3d gyro_corrected = imu_data.gyroscope - state_.segment<3>(12);  // bias_gyro\n        Eigen::Vector3d accel_corrected = imu_data.accelerometer - state_.segment<3>(12 + 3);  // bias_accel\n\n        // Integrate IMU measurements\n        integrate_imu(accel_corrected, gyro_corrected, dt);\n\n        last_imu_time_ = imu_data.timestamp;\n    }\n\n    void update_from_features(double current_time) {\n        // Use feature correspondences to update pose estimate\n        // This is a simplified implementation - full VIO would use more sophisticated methods\n\n        if (previous_features_.size() != current_features_.size()) return;\n\n        // Calculate motion estimate from feature correspondences\n        Eigen::Vector3d translation;\n        Eigen::Quaterniond rotation;\n\n        if (estimate_motion_from_features(translation, rotation)) {\n            // Update state with motion estimate\n            update_state_with_motion(translation, rotation);\n        }\n    }\n\n    void integrate_imu(const Eigen::Vector3d& corrected_accel,\n                      const Eigen::Vector3d& corrected_gyro,\n                      double dt) {\n        // Update orientation\n        Eigen::Vector4d quat_vec(state_.segment<4>(6));\n        Eigen::Quaterniond quat(quat_vec(3), quat_vec(0), quat_vec(1), quat_vec(2));\n\n        // Integrate quaternion derivative\n        Eigen::Quaterniond omega_quat(0.0, corrected_gyro.x(),\n                                     corrected_gyro.y(), corrected_gyro.z());\n        Eigen::Quaterniond quat_dot = 0.5 * (quat * omega_quat);\n\n        Eigen::Vector4d new_quat_vec = quat_vec + dt * Eigen::Vector4d(\n            quat_dot.x(), quat_dot.y(), quat_dot.z(), quat_dot.w());\n\n        Eigen::Quaterniond new_quat(new_quat_vec(3), new_quat_vec(0),\n                                   new_quat_vec(1), new_quat_vec(2));\n        new_quat.normalize();\n\n        // Update velocity and position\n        Eigen::Matrix3d rot_matrix = new_quat.toRotationMatrix();\n        Eigen::Vector3d gravity_in_body = rot_matrix.transpose() * gravity_world_;\n        Eigen::Vector3d acceleration = corrected_accel + gravity_in_body;\n\n        Eigen::Vector3d new_vel = state_.segment<3>(3) + dt * acceleration;\n        Eigen::Vector3d new_pos = state_.segment<3>(0) + dt * state_.segment<3>(3) +\n                                 0.5 * dt * dt * acceleration;\n\n        // Update state vector\n        state_.segment<3>(0) = new_pos;  // Position\n        state_.segment<3>(3) = new_vel;  // Velocity\n        state_.segment<4>(6) << new_quat.x(), new_quat.y(),\n                                new_quat.z(), new_quat.w();  // Orientation\n\n        // Update bias states (simplified random walk model)\n        // In practice, bias estimation is more complex\n    }\n\n    // Real-time safe accessors\n    Eigen::Vector3d get_position() const {\n        return state_.segment<3>(0);\n    }\n\n    Eigen::Quaterniond get_orientation() const {\n        Eigen::Quaterniond quat(state_(6 + 3), state_(6), state_(6 + 1), state_(6 + 2));\n        return quat;\n    }\n\n    Eigen::Vector3d get_velocity() const {\n        return state_.segment<3>(3);\n    }\n\n    Eigen::Vector3d get_gyro_bias() const {\n        return state_.segment<3>(12);\n    }\n\n    Eigen::Vector3d get_accel_bias() const {\n        return state_.segment<3>(12 + 3);\n    }\n\nprivate:\n    void detect_features(const cv::Mat& image) {\n        // Use Shi-Tomasi corner detection to find good features to track\n        std::vector<cv::Point2f> corners;\n\n        cv::goodFeaturesToTrack(image, corners, max_feature_count_, 0.01, 10);\n\n        current_features_ = corners;\n        feature_tracked_.assign(corners.size(), true);\n        feature_depth_.assign(corners.size(), -1.0);  // Unknown depth initially\n\n        // Assign unique IDs to features\n        feature_ids_.resize(corners.size());\n        static int next_feature_id = 0;\n        for (size_t i = 0; i < feature_ids_.size(); ++i) {\n            feature_ids_[i] = next_feature_id++;\n        }\n    }\n\n    void track_features(const cv::Mat& current_image) {\n        if (previous_features_.empty()) {\n            previous_features_ = current_features_;\n            return;\n        }\n\n        std::vector<uchar> status;\n        std::vector<float> error;\n\n        // Use Lucas-Kanade optical flow to track features\n        cv::calcOpticalFlowPyrLK(\n            previous_image_, current_image,\n            previous_features_, current_features_,\n            status, error\n        );\n\n        // Filter out lost features\n        std::vector<cv::Point2f> filtered_features;\n        std::vector<bool> filtered_tracked;\n        std::vector<double> filtered_depth;\n        std::vector<int> filtered_ids;\n\n        for (size_t i = 0; i < current_features_.size(); ++i) {\n            if (status[i] && error[i] < reprojection_threshold_) {\n                filtered_features.push_back(current_features_[i]);\n                filtered_tracked.push_back(true);\n                filtered_depth.push_back(feature_depth_[i]);\n                filtered_ids.push_back(feature_ids_[i]);\n            }\n        }\n\n        current_features_ = filtered_features;\n        feature_tracked_ = filtered_tracked;\n        feature_depth_ = filtered_depth;\n        feature_ids_ = filtered_ids;\n\n        // Store current image for next iteration\n        previous_image_ = current_image.clone();\n        previous_features_ = current_features_;\n    }\n\n    void add_new_features(const cv::Mat& image) {\n        if (current_features_.size() >= max_feature_count_) return;\n\n        // Detect new features in areas without existing features\n        cv::Mat mask = cv::Mat::ones(image.size(), CV_8UC1);\n\n        // Create mask to avoid areas near existing features\n        for (const auto& feature : current_features_) {\n            cv::circle(mask, feature, 30, cv::Scalar(0), -1);  // Mask out 30-pixel radius\n        }\n\n        std::vector<cv::Point2f> new_corners;\n        int needed_features = max_feature_count_ - current_features_.size();\n\n        cv::goodFeaturesToTrack(image, new_corners, needed_features, 0.01, 10, mask);\n\n        // Add new features\n        for (const auto& corner : new_corners) {\n            current_features_.push_back(corner);\n            feature_tracked_.push_back(true);\n            feature_depth_.push_back(-1.0);  // Unknown depth\n            static int next_feature_id = 0;\n            feature_ids_.push_back(next_feature_id++);\n        }\n    }\n\n    bool estimate_motion_from_features(Eigen::Vector3d& translation,\n                                     Eigen::Quaterniond& rotation) {\n        if (previous_features_.size() < 8) return false;  // Need minimum correspondences\n\n        // Use OpenCV's essential matrix estimation for motion\n        std::vector<uchar> mask;\n\n        cv::Mat essential = cv::findEssentialMat(\n            previous_features_, current_features_,\n            focal_length_, principal_point_,\n            cv::RANSAC, 0.999, 1.0, mask\n        );\n\n        if (essential.empty()) return false;\n\n        // Decompose essential matrix to get rotation and translation\n        cv::Mat R, t;\n        int inliers = cv::recoverPose(essential, previous_features_, current_features_,\n                                     R, t, focal_length_, principal_point_);\n\n        if (inliers < 8) return false;\n\n        // Convert to Eigen\n        rotation = Eigen::Matrix3d(R);\n        translation << t.at<double>(0), t.at<double>(1), t.at<double>(2);\n\n        // Normalize translation to unit vector if needed\n        if (translation.norm() > 0.001) {  // Avoid division by zero\n            translation.normalize();\n        }\n\n        return true;\n    }\n\n    void update_state_with_motion(const Eigen::Vector3d& translation,\n                                 const Eigen::Quaterniond& rotation) {\n        // Apply motion estimate to update state\n        // This is a simplified approach - full VIO uses optimization-based methods\n\n        // Update orientation\n        Eigen::Quaterniond current_quat(state_(6 + 3), state_(6),\n                                       state_(6 + 1), state_(6 + 2));\n        Eigen::Quaterniond new_quat = current_quat * rotation;\n        new_quat.normalize();\n\n        state_.segment<4>(6) << new_quat.x(), new_quat.y(),\n                                new_quat.z(), new_quat.w();\n\n        // Update position based on translation\n        state_.segment<3>(0) += translation * 0.1;  // Scale appropriately\n    }\n\n    cv::Mat previous_image_;\n};\n\n} // namespace humanoid_sensors\n"})}),"\n",(0,a.jsx)(n.h2,{id:"multi-sensor-integration-architecture",children:"Multi-Sensor Integration Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-fusion-framework-for-humanoid-robots",children:"Sensor Fusion Framework for Humanoid Robots"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:"// sensor_fusion_framework.cpp - Multi-sensor fusion framework for humanoid robots\n#include <Eigen/Dense>\n#include <vector>\n#include <map>\n#include <memory>\n#include <chrono>\n#include <mutex>\n\nnamespace humanoid_sensors {\n\n// Sensor types for humanoid robots\nenum class SensorType {\n    IMU,\n    CAMERA,\n    ENCODER,\n    FORCE_TORQUE,\n    LIDAR,\n    GPS,\n    BAROMETER\n};\n\n// Sensor data structure\nstruct SensorData {\n    SensorType type;\n    Eigen::VectorXd measurement;\n    double timestamp;\n    int sensor_id;\n    Eigen::MatrixXd covariance;  // Measurement uncertainty\n};\n\n// State vector for humanoid: position, orientation, velocity, angular velocity, joint positions, joint velocities\nstruct RobotState {\n    Eigen::Vector3d position;           // World position\n    Eigen::Quaterniond orientation;     // World orientation\n    Eigen::Vector3d linear_velocity;    // World linear velocity\n    Eigen::Vector3d angular_velocity;   // Body angular velocity\n    std::vector<double> joint_positions;    // Joint positions\n    std::vector<double> joint_velocities;   // Joint velocities\n    Eigen::Vector3d com_position;       // Center of mass position\n    Eigen::Vector3d com_velocity;       // Center of mass velocity\n    double timestamp;\n\n    RobotState(int num_joints = 0) : joint_positions(num_joints, 0.0),\n                                   joint_velocities(num_joints, 0.0) {}\n};\n\nclass SensorFusionFramework {\nprivate:\n    // Estimators for different sensor combinations\n    std::unique_ptr<EKFStateEstimator> ekf_estimator_;\n    std::unique_ptr<UKFStateEstimator> ukf_estimator_;\n    std::unique_ptr<VIOEstimator> vio_estimator_;\n\n    // Sensor data buffers for time synchronization\n    std::map<SensorType, std::deque<SensorData>> sensor_buffers_;\n    std::mutex buffer_mutex_;\n\n    // Robot state\n    RobotState current_state_;\n    Eigen::MatrixXd state_covariance_;\n\n    // Time synchronization parameters\n    double time_sync_window_;  // Window for temporal alignment\n    bool initialized_;\n\n    // Joint information\n    int num_joints_;\n\npublic:\n    SensorFusionFramework(int num_joints)\n        : num_joints_(num_joints), time_sync_window_(0.01), initialized_(false) {\n\n        current_state_ = RobotState(num_joints);\n        state_covariance_ = Eigen::MatrixXd::Identity(13, 13) * 0.1;  // Simplified state\n\n        // Initialize estimators\n        ekf_estimator_ = std::make_unique<EKFStateEstimator>();\n        ukf_estimator_ = std::make_unique<UKFStateEstimator>();\n        // vio_estimator_ would need camera parameters to initialize\n    }\n\n    void process_sensor_data(const SensorData& sensor_data) {\n        std::lock_guard<std::mutex> lock(buffer_mutex_);\n\n        // Add to appropriate buffer\n        sensor_buffers_[sensor_data.type].push_back(sensor_data);\n\n        // Clean old data outside sync window\n        clean_old_data(sensor_data.timestamp);\n\n        // Attempt fusion when we have compatible sensor data\n        attempt_fusion(sensor_data.timestamp);\n    }\n\n    void add_imu_data(const Eigen::Vector3d& accel,\n                     const Eigen::Vector3d& gyro,\n                     double timestamp) {\n\n        SensorData imu_data;\n        imu_data.type = SensorType::IMU;\n        imu_data.measurement.resize(6);\n        imu_data.measurement << accel, gyro;\n        imu_data.timestamp = timestamp;\n        imu_data.sensor_id = 0;  // Primary IMU\n        imu_data.covariance = Eigen::MatrixXd::Identity(6, 6) * 0.01;\n\n        process_sensor_data(imu_data);\n    }\n\n    void add_encoder_data(const std::vector<double>& joint_positions,\n                         const std::vector<double>& joint_velocities,\n                         double timestamp) {\n\n        int n = joint_positions.size();\n        SensorData encoder_data;\n        encoder_data.type = SensorType::ENCODER;\n        encoder_data.measurement.resize(2 * n);  // Positions and velocities\n        for (int i = 0; i < n; ++i) {\n            encoder_data.measurement(i) = joint_positions[i];\n            encoder_data.measurement(i + n) = joint_velocities[i];\n        }\n        encoder_data.timestamp = timestamp;\n        encoder_data.covariance = Eigen::MatrixXd::Identity(2 * n, 2 * n) * 0.001;\n\n        process_sensor_data(encoder_data);\n    }\n\n    void add_force_torque_data(const Eigen::Vector3d& force,\n                              const Eigen::Vector3d& torque,\n                              int foot_id,  // 0 for left foot, 1 for right foot\n                              double timestamp) {\n\n        SensorData ft_data;\n        ft_data.type = SensorType::FORCE_TORQUE;\n        ft_data.measurement.resize(6);  // Force + Torque\n        ft_data.measurement << force, torque;\n        ft_data.timestamp = timestamp;\n        ft_data.sensor_id = foot_id;\n        ft_data.covariance = Eigen::MatrixXd::Identity(6, 6) * 0.1;\n\n        process_sensor_data(ft_data);\n    }\n\n    RobotState get_fused_state() const {\n        std::lock_guard<std::mutex> lock(buffer_mutex_);\n        return current_state_;\n    }\n\n    Eigen::MatrixXd get_state_covariance() const {\n        std::lock_guard<std::mutex> lock(buffer_mutex_);\n        return state_covariance_;\n    }\n\n    // Multi-rate fusion: different sensors update at different rates\n    void attempt_fusion(double current_time) {\n        // Check if we have data for fusion\n        bool has_imu = !sensor_buffers_[SensorType::IMU].empty();\n        bool has_encoders = !sensor_buffers_[SensorType::ENCODER].empty();\n        bool has_ft = !sensor_buffers_[SensorType::FORCE_TORQUE].empty();\n\n        if (has_imu) {\n            // Use IMU for continuous prediction updates\n            auto imu_data = sensor_buffers_[SensorType::IMU].back();\n\n            // Convert to EKF IMU format\n            EKFStateEstimator::IMUData ekf_imu;\n            ekf_imu.accelerometer = imu_data.measurement.head(3);\n            ekf_imu.gyroscope = imu_data.measurement.tail(3);\n            ekf_imu.timestamp = imu_data.timestamp;\n\n            ekf_estimator_->predict(ekf_imu);\n        }\n\n        if (has_encoders) {\n            // Use encoder data for position updates\n            auto encoder_data = sensor_buffers_[SensorType::ENCODER].back();\n\n            // Update joint state\n            int n = num_joints_;\n            for (int i = 0; i < n && i < current_state_.joint_positions.size(); ++i) {\n                current_state_.joint_positions[i] = encoder_data.measurement(i);\n                current_state_.joint_velocities[i] = encoder_data.measurement(i + n);\n            }\n\n            // If we have position sensors, update position estimate\n            // This would involve more complex kinematic calculations in practice\n        }\n\n        if (has_ft && has_imu) {\n            // Use force/torque sensors for contact detection and state refinement\n            auto ft_data = sensor_buffers_[SensorType::FORCE_TORQUE].back();\n            auto imu_data = sensor_buffers_[SensorType::IMU].back();\n\n            // Detect contact and update state based on contact constraints\n            if (std::abs(ft_data.measurement.head(3).norm()) > 10.0) {  // Contact threshold\n                // Apply contact constraints to state estimate\n                apply_contact_constraints(ft_data, imu_data);\n            }\n        }\n\n        // Update current state from estimator\n        if (ekf_estimator_) {\n            current_state_.position = ekf_estimator_->get_position();\n            current_state_.orientation = ekf_estimator_->get_orientation();\n            current_state_.linear_velocity = ekf_estimator_->get_velocity();\n            current_state_.angular_velocity = ekf_estimator_->get_angular_velocity();\n            state_covariance_ = ekf_estimator_->get_covariance();\n        }\n    }\n\nprivate:\n    void clean_old_data(double current_time) {\n        // Remove sensor data older than the sync window\n        for (auto& [sensor_type, buffer] : sensor_buffers_) {\n            while (!buffer.empty() &&\n                   (current_time - buffer.front().timestamp) > time_sync_window_) {\n                buffer.pop_front();\n            }\n        }\n    }\n\n    void apply_contact_constraints(const SensorData& ft_data,\n                                  const SensorData& imu_data) {\n        // Apply contact constraints to refine state estimate\n        // This is where Zero Moment Point (ZMP) calculations and balance constraints would be applied\n\n        // Example: Use force/torque data to refine center of pressure estimate\n        Eigen::Vector3d contact_force = ft_data.measurement.head(3);\n        Eigen::Vector3d contact_torque = ft_data.measurement.tail(3);\n\n        // Calculate center of pressure (simplified)\n        double fz = contact_force(2);\n        if (std::abs(fz) > 0.1) {  // Avoid division by zero\n            double cop_x = -contact_torque(1) / fz;\n            double cop_y = contact_torque(0) / fz;\n\n            // Use this information to refine position estimate\n            // In practice, this would involve more sophisticated contact-aided estimation\n        }\n    }\n};\n\n// Real-time safe sensor fusion interface for humanoid control\nclass RealTimeSensorFusion {\nprivate:\n    std::unique_ptr<SensorFusionFramework> fusion_framework_;\n    mutable std::mutex state_mutex_;\n    RobotState latest_state_;\n\npublic:\n    RealTimeSensorFusion(int num_joints) {\n        fusion_framework_ = std::make_unique<SensorFusionFramework>(num_joints);\n    }\n\n    // Thread-safe state access for real-time control\n    RobotState get_state() const {\n        std::lock_guard<std::mutex> lock(state_mutex_);\n        return latest_state_;\n    }\n\n    // Non-blocking sensor data input\n    void add_sensor_data(const SensorData& sensor_data) {\n        // Process in background or queue for processing\n        // In real implementation, this might use lock-free queues\n        fusion_framework_->process_sensor_data(sensor_data);\n\n        // Update latest state\n        std::lock_guard<std::mutex> lock(state_mutex_);\n        latest_state_ = fusion_framework_->get_fused_state();\n    }\n\n    // Specialized methods for common sensor types\n    void add_imu_data(const Eigen::Vector3d& accel,\n                     const Eigen::Vector3d& gyro,\n                     double timestamp) {\n        fusion_framework_->add_imu_data(accel, gyro, timestamp);\n\n        std::lock_guard<std::mutex> lock(state_mutex_);\n        latest_state_ = fusion_framework_->get_fused_state();\n    }\n\n    void add_encoder_data(const std::vector<double>& joint_positions,\n                         const std::vector<double>& joint_velocities,\n                         double timestamp) {\n        fusion_framework_->add_encoder_data(joint_positions, joint_velocities, timestamp);\n\n        std::lock_guard<std::mutex> lock(state_mutex_);\n        latest_state_ = fusion_framework_->get_fused_state();\n    }\n};\n\n} // namespace humanoid_sensors\n"})}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(n.p,{children:["Sensor fusion for humanoid robotics combines data from multiple sensors to provide robust, accurate state estimates essential for balance control, navigation, and manipulation. ",(0,a.jsx)(n.strong,{children:"Kalman filtering"})," provides optimal state estimation by combining motion models with sensor measurements, while ",(0,a.jsx)(n.strong,{children:"Visual-Inertial Odometry"})," integrates camera and IMU data for accurate pose estimation. ",(0,a.jsx)(n.strong,{children:"Multi-sensor architectures"})," coordinate diverse sensor types with different update rates and characteristics."]}),"\n",(0,a.jsx)(n.p,{children:"Key capabilities include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Extended/Unscented Kalman Filters"})," for nonlinear state estimation with uncertainty quantification"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual-Inertial Odometry"})," for robust localization using complementary visual and inertial sensing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-rate fusion"})," techniques that handle sensors with different update frequencies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time sensor fusion"})," frameworks optimized for control system integration"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"In Chapter 12, we will transition to Module 4: Humanoid Integration, beginning with whole-body control strategies that coordinate multiple subsystems for complex humanoid behaviors."}),"\n",(0,a.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Conceptual"}),": Compare the computational complexity and accuracy of EKF vs UKF for humanoid state estimation. When would you choose one over the other?"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Applied"}),": Implement a real-time sensor fusion system that integrates IMU, encoder, and force/torque sensors for humanoid balance control with < 5ms update time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Structural"}),": Analyze the observability properties of different sensor combinations for humanoid state estimation (e.g., IMU-only, IMU+encoders, full sensor suite)."]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);